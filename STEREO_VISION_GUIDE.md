# 🥽 VR 立体视觉完整指南

## 📋 问题总结

### 已解决的问题
1. ✅ WebXR 立体渲染正常（立方体是立体的）
2. ✅ 图层系统正常（左眼看左边，右眼看右边）

### 当前问题
1. ❌ **两个视频流都显示红色 "LEFT EYE"** → 视频流顺序/识别问题
2. ❌ **视频看起来是 2D 平面** → 需要正确的立体视觉设置

---

## 🔍 根本原因分析

### 问题 1: 视频流顺序不确定

**原因**：WebRTC 的 `ontrack` 事件接收顺序是**不确定的**！

即使服务器按照 "左眼 → 右眼" 的顺序添加轨道：
```python
self.pc.addTrack(left_track)   # 先添加左眼
self.pc.addTrack(right_track)  # 后添加右眼
```

客户端接收时可能是：
- 情况 A: 先收到左眼，后收到右眼 ✅
- 情况 B: 先收到右眼，后收到左眼 ❌
- 情况 C: 两个都是左眼（右眼还没到达）❌

**解决方案**：通过分析视频内容（颜色）来识别左右眼

### 问题 2: 视频立体感

**原因**：立体视觉需要三个要素：

1. ✅ **双目视差**：左右眼看到略微不同的画面
   - 机器人相机已经有 IPD = 64mm
   - 左右眼图像确实不同

2. ✅ **正确的图层分离**：每只眼睛看到对应的图像
   - 图层系统已经工作
   - 左眼看图层 1，右眼看图层 2

3. ❌ **合适的显示方式**：视频应该填满视野，而不是显示为空间中的平板
   - 之前：视频显示在空间中的平面上（像一个电视屏幕）
   - 现在：视频填满整个视野（像真的在机器人眼睛里看）

---

## 🛠️ 最新修复

### 修复 1: 智能视频流识别

**文件**: `main.js`

**功能**：
- 接收两个视频流后，分析每个流的颜色
- 红色 (R > B) → 左眼
- 蓝色 (B > R) → 右眼
- 自动正确分配到 `leftTexture` 和 `rightTexture`

**关键代码**：
```javascript
async function identifyEyeStreams(stream1, stream2) {
    // 创建临时 video 元素
    // 绘制到 canvas
    // 分析平均 RGB 值
    // 判断哪个是左眼，哪个是右眼
}
```

**预期输出**：
```
🎨 视频流颜色分析:
   - 流 1: R=xxx, G=xxx, B=xxx
   - 流 2: R=xxx, G=xxx, B=xxx
✅ 识别结果: 流1=左眼(红), 流2=右眼(蓝)
```

### 修复 2: 全视野视频显示

**文件**: `src/vr-scene.js`

**改动**：
- 创建大的平面屏幕，填满 90° 视场角
- 距离相机 1 米
- 禁用深度测试，作为背景渲染
- 隐藏测试用的立方体和地板

**效果**：
- 视频不再是空间中的"电视屏幕"
- 视频填满整个视野
- 就像真的在机器人眼睛里看一样

---

## 🚀 测试步骤

### 步骤 1: 确保虚拟机器人在测试图案模式下运行

```bash
cd virtual-robot
python main.py --test-pattern
```

**预期输出**：
```
✅ 视频轨道创建: left 眼, 30 fps, 模式: 测试图案
✅ 视频轨道创建: right 眼, 30 fps, 模式: 测试图案
📹 添加左眼轨道: ID=xxx
📹 添加右眼轨道: ID=xxx
✅ 已添加 2 个视频轨道（左眼 + 右眼），模式: 测试图案
   ⚠️  注意: WebRTC 轨道接收顺序可能不确定！
```

### 步骤 2: 刷新 PICO VR 浏览器

访问: `https://172.20.10.2:3000`

### 步骤 3: 查看浏览器控制台（进入 VR 前）

**应该看到**：
```
📹 收到视频轨道 1: xxx
   - Track label: xxx
   - Stream ID: xxx
📹 收到视频轨道 2: xxx
   - Track label: xxx
   - Stream ID: xxx
🔍 正在识别左右眼视频流...
🎨 视频流颜色分析:
   - 流 1: R=xxx, G=xxx, B=xxx
   - 流 2: R=xxx, G=xxx, B=xxx
✅ 识别结果: 流1=左眼(红), 流2=右眼(蓝)
   或
✅ 识别结果: 流1=右眼(蓝), 流2=左眼(红)
✅ 双目视频流已连接！可以进入 VR 了
```

**关键检查点**：
- ✅ 必须看到 "识别结果"
- ✅ 必须正确识别红色和蓝色

### 步骤 4: 进入 VR 模式

点击 "ENTER VR"

**控制台应该显示**：
```
🥽 VR 会话已启动
🥽 配置立体图层...
📷 XR 相机数量: 2
   - 左眼相机 -> 图层 0 + 1
   - 右眼相机 -> 图层 0 + 2
✅ 双目图层配置完成
```

### 步骤 5: 验证立体效果

**测试图案模式下**：

1. **睁开双眼**：
   - 应该看到红色和蓝色混合的画面
   - 或者红蓝交替闪烁

2. **闭上左眼，只用右眼看**：
   - 应该只看到蓝色 "RIGHT EYE"

3. **闭上右眼，只用左眼看**：
   - 应该只看到红色 "LEFT EYE"

4. **视野范围**：
   - 视频应该填满整个视野
   - 不应该看到空间中的"屏幕边框"

---

## 📊 成功标志

### ✅ 视频流识别成功

**控制台**：
```
✅ 识别结果: 流1=左眼(红), 流2=右眼(蓝)
```

**VR 显示**：
- 左眼看到红色
- 右眼看到蓝色

### ✅ 立体视觉成功

**真实场景模式下**（停止 `--test-pattern`）：
- 左右眼看到略微不同的画面
- 有明显的深度感
- 物体有立体感（不是平面）

---

## 🔧 故障排除

### 问题 A: 仍然看到两个红色

**可能原因**：
1. 视频流识别失败
2. 右眼视频流没有正确传输

**检查**：
- 查看控制台的 "🎨 视频流颜色分析"
- 如果两个流的颜色都是红色 → 服务器端问题
- 如果识别结果错误 → 客户端识别逻辑问题

**解决**：
```bash
# 重启虚拟机器人服务器
cd virtual-robot
python main.py --test-pattern
```

### 问题 B: 视频仍然是平面

**可能原因**：
1. 机器人相机的 IPD 设置不正确
2. 左右眼图像完全相同

**检查**：
```bash
# 保存相机图像到文件
cd virtual-robot
python debug_camera.py

# 查看 debug_output/stereo_comparison.png
# 左右眼图像应该略有不同
```

**解决**：
- 检查 `virtual-robot/stereo_camera.py` 中的 IPD 设置
- 确保左右眼相机位置不同

### 问题 C: 看不到任何视频

**可能原因**：
1. 图层配置失败
2. 视频纹理没有更新

**检查**：
- 控制台是否显示 "✅ 双目图层配置完成"
- XR 相机数量是否为 2

**解决**：
- 刷新浏览器
- 重启虚拟机器人服务器

---

## 💡 下一步

### 如果测试图案成功

1. **停止测试图案模式**：
   ```bash
   # 按 Ctrl+C 停止
   # 重新启动（不带 --test-pattern）
   python main.py
   ```

2. **刷新浏览器**

3. **进入 VR**

4. **享受真实的机器人视角！**

### 如果仍然有问题

请提供以下信息：
1. 浏览器控制台的完整输出（特别是颜色分析部分）
2. 虚拟机器人服务器的输出
3. 你在 VR 中看到的具体内容
4. `debug_output/stereo_comparison.png` 的截图

---

## 📚 技术细节

### 立体视觉原理

1. **双目视差**：
   - 人眼间距（IPD）约 64mm
   - 左右眼看到的画面略有不同
   - 大脑融合两个画面，产生深度感

2. **VR 中的实现**：
   - 左眼相机位置：`(x - IPD/2, y, z)`
   - 右眼相机位置：`(x + IPD/2, y, z)`
   - 渲染两次场景，每次用不同的相机
   - 左眼看左眼图像，右眼看右眼图像

3. **WebXR 图层系统**：
   - 图层 0：默认场景（地板、立方体等）
   - 图层 1：左眼专属内容
   - 图层 2：右眼专属内容
   - 左眼相机：`layers.enable(0)` + `layers.enable(1)`
   - 右眼相机：`layers.enable(0)` + `layers.enable(2)`

### 视频流识别算法

```javascript
// 1. 创建临时 video 元素
const video = document.createElement('video');
video.srcObject = stream;
video.play();

// 2. 等待视频加载
await waitForVideoReady(video);

// 3. 绘制到 canvas
const canvas = document.createElement('canvas');
const ctx = canvas.getContext('2d');
ctx.drawImage(video, 0, 0);

// 4. 获取像素数据
const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

// 5. 计算平均颜色
const avgColor = analyzeColor(imageData.data);

// 6. 判断左右眼
if (avgColor.r > avgColor.b) {
    // 红色 → 左眼
} else {
    // 蓝色 → 右眼
}
```

---

## 🎯 总结

**核心问题**：
1. WebRTC 轨道接收顺序不确定 → 需要智能识别
2. 视频显示方式不对 → 需要填满视野

**解决方案**：
1. 颜色分析识别左右眼
2. 全视野视频显示

**预期效果**：
- 左眼看到左眼视频（红色测试图案）
- 右眼看到右眼视频（蓝色测试图案）
- 视频填满整个视野
- 真实场景下有立体感

